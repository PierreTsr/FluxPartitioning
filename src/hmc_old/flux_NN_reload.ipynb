{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Flux Partitioning with uncertainty prediction\n",
    "\n",
    "This notebook aims at providing an uncertainty estimation for the flux partitioning problem, using Bayesian Neural Networks. This notebook only loads parameters from a previous run of HMC to plot some visulizations.\n",
    "Thanks to Mohamed Aziz Bhouri for his help with Bayesian modeling and the HMC fine-tuning, and to Weiwei Zhan for her flux partitioning NN model and her data.\n",
    "\n",
    "## Initialization and pre-training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "from HMC import HMC\n",
    "from flux_preprocessing import load_dataset\n",
    "from flux_utils import get_layer_model, fluxes_SIF_predict_noSIF, count_out_distribution\n",
    "from flux_viz import quad_viz\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"legend.frameon\"] = False\n",
    "plt.rcParams['savefig.dpi'] = 310\n",
    "plt.rcParams['font.size'] = 13\n",
    "data_dir = Path(\"../data\")\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "hidden_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, test, EV1_train1, EV2_train1, NEE_train1, label_train, EV1_test1, EV2_test1, NEE_test1, label_test, NEE_max_abs = load_dataset(\n",
    "    'NNinput_SCOPE_US_Ha1_1314.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def NN_noSIF(n_neuron, activation1, activation2):\n",
    "    # GPP\n",
    "    APAR_input = Input(shape=(1,), dtype='float32', name='APAR_input')\n",
    "    EV_input1 = Input(shape=(EV1_train1.shape[1],), dtype='float32', name='EV_input1')\n",
    "    x = Dense(n_neuron, activation=activation1, name='hidden1_1')(EV_input1)\n",
    "    # x = Dense(n_neuron, activation=activation2, name='hidden1_2')(x)\n",
    "    ln_GPP = Dense(1, activation=None, name='ln_GPP')(x)\n",
    "    GPP_1 = Lambda(lambda x: K.exp(x), name='GPP_1')(ln_GPP)\n",
    "    GPP = keras.layers.Multiply(name='GPP')([GPP_1, APAR_input])\n",
    "\n",
    "    # Reco\n",
    "    EV_input2 = Input(shape=(EV2_train1.shape[1],), dtype='float32', name='EV_input2')\n",
    "    x = Dense(n_neuron, activation=activation1, name='hidden2_1')(EV_input2)\n",
    "    x = Dense(n_neuron, activation=activation2, name='hidden2_2')(x)\n",
    "    ln_Reco = Dense(1, activation=None, name='ln_Reco')(x)\n",
    "    Reco = Lambda(lambda x: K.exp(x), name='Reco')(ln_Reco)\n",
    "\n",
    "    NEE = keras.layers.Subtract(name='NEE')([Reco, GPP])\n",
    "\n",
    "    model_NEE = Model(inputs=[APAR_input, EV_input1, EV_input2], outputs=[NEE])\n",
    "    model_NEE.compile(\n",
    "        optimizer=keras.optimizers.Adam(5e-3),\n",
    "        loss=keras.losses.MeanSquaredError()\n",
    "    )\n",
    "\n",
    "    return model_NEE"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "parameters_full = np.load(\"../etc/diagnostic/flux_nn/full_parameters.npy\").astype(np.float32)\n",
    "log_gamma_full = np.load(\"../etc/diagnostic/flux_nn/log_gamma.npy\").astype(np.float32)\n",
    "log_lambda_full = np.load(\"../etc/diagnostic/flux_nn/log_lambda.npy\").astype(np.float32)\n",
    "log_likelihood_full = np.load(\"../etc/diagnostic/flux_nn/log_likelihood.npy\").astype(np.float32)\n",
    "\n",
    "parameters, parameters2 = parameters_full[:,:,0], parameters_full[:,:,1]\n",
    "log_gamma, log_gamma2 = log_gamma_full[:,0], log_gamma_full[:,1]\n",
    "log_lambda, log_lambda2 = log_lambda_full[:,0], log_lambda_full[:,1]\n",
    "log_likelihood, log_likelihood2 = log_likelihood_full[:,0], log_likelihood_full[:,1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is a brief overview of the model that we are using. Depending on the specifications provided in the above function, the number  of trainable parameters is in the $10^3:10^4$ range."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model1 = NN_noSIF(32,\"relu\",\"relu\")\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HMC run\n",
    "\n",
    "Let's check on the MAP estimate, and compare the results with the training set.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Computing MAP estimate\n",
    "n_obs_train = EV1_train1.shape[0]\n",
    "n_obs_test = EV1_test1.shape[0]\n",
    "hmc = HMC(model1, L=400, epsilon_min=5e-4, epsilon_max=5e-4, batch_size=n_obs_train, n_obs=n_obs_train)\n",
    "idx_MAP = np.argmin(log_likelihood)\n",
    "MAP = tf.constant(parameters[idx_MAP, :], dtype=tf.float32)\n",
    "hmc.set_model_params(MAP)\n",
    "NEE_train_MAP, GPP_train_MAP, Reco_train_MAP = fluxes_SIF_predict_noSIF(hmc.model, label_train, EV1_train1, EV2_train1,\n",
    "                                                                        NEE_max_abs)\n",
    "NEE_test_MAP, GPP_test_MAP, Reco_test_MAP = fluxes_SIF_predict_noSIF(hmc.model, label_test, EV1_test1, EV2_test1,\n",
    "                                                                     NEE_max_abs)\n",
    "\n",
    "train_pred = hmc.model({'APAR_input': label_train, 'EV_input1': EV1_train1, 'EV_input2': EV2_train1})\n",
    "train_loss = hmc.model.loss(train_pred, NEE_train1)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(train_pred, NEE_train1)\n",
    "plt.show()\n",
    "\n",
    "test_pred = hmc.model({'APAR_input': label_test, 'EV_input1': EV1_test1, 'EV_input2': EV2_test1})\n",
    "test_loss = hmc.model.loss(test_pred, NEE_test1)\n",
    "print(\"MAP training loss = {train_loss:.3e} and testing loss = {test_loss:.3e}\".format(train_loss = train_loss, test_loss=test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HMC sampling\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# preparing sampling\n",
    "precision = np.exp(log_gamma)\n",
    "n_samples = 500\n",
    "n_iter = parameters_full.shape[0]\n",
    "NEE_train_traj, GPP_train_traj, Reco_train_traj = np.zeros((n_obs_train, n_samples)), np.zeros(\n",
    "    (n_obs_train, n_samples)), np.zeros(\n",
    "    (n_obs_train, n_samples))\n",
    "NEE_test_traj, GPP_test_traj, Reco_test_traj = np.zeros((n_obs_test, n_samples)), np.zeros(\n",
    "    (n_obs_test, n_samples)), np.zeros(\n",
    "    (n_obs_test, n_samples))\n",
    "sigma_NEE = np.std(train[\"NEE_canopy\"])\n",
    "sigma_GPP = np.std(train[\"GPP_canopy\"])\n",
    "sigma_Reco = np.std(train[\"Reco_canopy\"])\n",
    "\n",
    "# sampling\n",
    "model_NEE = get_layer_model(hmc.model, \"NEE\")\n",
    "model_GPP = get_layer_model(hmc.model, \"GPP\")\n",
    "model_Reco = get_layer_model(hmc.model, \"Reco\")\n",
    "for k in tqdm(range(n_samples)):\n",
    "    idx_1 = np.random.randint(0, n_iter)\n",
    "    idx_2 = np.random.randint(0, n_iter)\n",
    "    w_sample = parameters[-idx_1, :]\n",
    "    precision_here = precision[-idx_1]\n",
    "    hmc.set_model_params(w_sample)\n",
    "\n",
    "    NEE_train_traj[:, k] = NEE_max_abs * tf.squeeze(model_NEE({'APAR_input': label_train,\n",
    "                                                               'EV_input1': EV1_train1,\n",
    "                                                               'EV_input2': EV2_train1})) + sigma_NEE * np.random.normal() / np.sqrt(\n",
    "        precision_here)\n",
    "    GPP_train_traj[:, k] = NEE_max_abs * tf.squeeze(model_GPP({'APAR_input': label_train,\n",
    "                                                               'EV_input1': EV1_train1,\n",
    "                                                               'EV_input2': EV2_train1})) + sigma_GPP * np.random.normal() / np.sqrt(\n",
    "        precision_here)\n",
    "    Reco_train_traj[:, k] = NEE_max_abs * tf.squeeze(model_Reco({'APAR_input': label_train,\n",
    "                                                                 'EV_input1': EV1_train1,\n",
    "                                                                 'EV_input2': EV2_train1})) + sigma_Reco * np.random.normal() / np.sqrt(\n",
    "        precision_here)\n",
    "\n",
    "    NEE_test_traj[:, k] = NEE_max_abs * tf.squeeze(model_NEE({'APAR_input': label_test,\n",
    "                                                              'EV_input1': EV1_test1,\n",
    "                                                              'EV_input2': EV2_test1})) + sigma_NEE * np.random.normal() / np.sqrt(\n",
    "        precision_here)\n",
    "    GPP_test_traj[:, k] = NEE_max_abs * tf.squeeze(model_GPP({'APAR_input': label_test,\n",
    "                                                              'EV_input1': EV1_test1,\n",
    "                                                              'EV_input2': EV2_test1})) + sigma_GPP * np.random.normal() / np.sqrt(\n",
    "        precision_here)\n",
    "    Reco_test_traj[:, k] = NEE_max_abs * tf.squeeze(model_Reco({'APAR_input': label_test,\n",
    "                                                                'EV_input1': EV1_test1,\n",
    "                                                                'EV_input2': EV2_test1})) + sigma_Reco * np.random.normal() / np.sqrt(\n",
    "        precision_here)\n",
    "\n",
    "mu_NEE_pred_train = np.mean(NEE_train_traj, axis=1)\n",
    "mu_GPP_pred_train = np.mean(GPP_train_traj, axis=1)\n",
    "mu_Reco_pred_train = np.mean(Reco_train_traj, axis=1)\n",
    "\n",
    "sigma_NEE_pred_train = np.std(NEE_train_traj, axis=1)\n",
    "sigma_GPP_pred_train = np.std(GPP_train_traj, axis=1)\n",
    "sigma_Reco_pred_train = np.std(Reco_train_traj, axis=1)\n",
    "\n",
    "mu_NEE_pred_test = np.mean(NEE_test_traj, axis=1)\n",
    "mu_GPP_pred_test = np.mean(GPP_test_traj, axis=1)\n",
    "mu_Reco_pred_test = np.mean(Reco_test_traj, axis=1)\n",
    "\n",
    "sigma_NEE_pred_test = np.std(NEE_test_traj, axis=1)\n",
    "sigma_GPP_pred_test = np.std(GPP_test_traj, axis=1)\n",
    "sigma_Reco_pred_test = np.std(Reco_test_traj, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Diagnostics\n",
    "\n",
    "I run some diagnostics here (mostly auto-correlation). A more complete set of diagnostics is conducted in the corresponding R notebook.\n",
    "The first step is to conduct a second HMC walk, with a different initialization, to check on the convergence.\n",
    "\n",
    "Let's then check with 16 random parameters that the auto-correlation plot doesn't show any symptomatic behavior."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot auto-correlation\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(12, 10))\n",
    "axs = axs.flatten()\n",
    "max_lag = int(n_iter/4)\n",
    "for k in range(len(axs)):\n",
    "    i = np.random.randint(0, parameters.shape[1])\n",
    "    auto_cor = []\n",
    "    mean = np.mean(parameters[:, i])\n",
    "    var = np.var(parameters[:, i])\n",
    "    for lag in range(1, max_lag):\n",
    "        param = parameters[lag:, i]\n",
    "        param_lagged = parameters[:-lag, i]\n",
    "        auto_cor.append(np.mean((param - mean) * (param_lagged - mean)) / var)\n",
    "    axs[k].plot(np.arange(1, max_lag), auto_cor)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We conduct the same operation with $\\log(\\gamma)$ and $\\log(\\lambda)$. These are more prone to show high-correlation, especially $\\log(\\lambda)$, as explained earlier. If that's the case, I would suggest to slightly increase `epsilon` as long as the acceptance rate doesn't drop, and then increase `L`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "mean = np.mean(log_gamma)\n",
    "var = np.var(log_gamma)\n",
    "auto_cor = []\n",
    "for lag in range(1, max_lag):\n",
    "    param = log_gamma[lag:]\n",
    "    param_lagged = log_gamma[:-lag]\n",
    "    auto_cor.append(np.mean((param - mean) * (param_lagged - mean)) / var)\n",
    "axs[0].plot(np.arange(1, max_lag), auto_cor)\n",
    "\n",
    "mean = np.mean(log_lambda)\n",
    "var = np.var(log_lambda)\n",
    "auto_cor = []\n",
    "for lag in range(1, max_lag):\n",
    "    param = log_lambda[lag:]\n",
    "    param_lagged = log_lambda[:-lag]\n",
    "    auto_cor.append(np.mean((param - mean) * (param_lagged - mean)) / var)\n",
    "axs[1].plot(np.arange(1, max_lag), auto_cor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also have a look at the parameter spread and correlation. It should be close to a centered gaussian in most cases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot parameters spread\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 15))\n",
    "for i in range(axs.shape[0]):\n",
    "    axs[i, i].hist(parameters[:, i], bins=30, density=True)\n",
    "    for j in range(i):\n",
    "        axs[i, j].scatter(parameters[:, j], parameters[:, i], s=1, marker=\"+\", c=np.arange(parameters.shape[0]))\n",
    "    for j in range(i + 1, axs.shape[0]):\n",
    "        axs[i, j].hist2d(parameters[:, j], parameters[:, i], bins=30, cmap=\"Blues\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predictions\n",
    "\n",
    "Let's now have a look at the computed predictions and uncertainties."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Logging predictions in their respective dataframes\n",
    "test['NEE_MAP'] = NEE_test_MAP\n",
    "test['Reco_MAP'] = Reco_test_MAP\n",
    "test['GPP_MAP'] = GPP_test_MAP\n",
    "train['NEE_MAP'] = NEE_train_MAP\n",
    "train['Reco_MAP'] = Reco_train_MAP\n",
    "train['GPP_MAP'] = GPP_train_MAP\n",
    "\n",
    "test['NEE_mean'] = mu_NEE_pred_test\n",
    "test['Reco_mean'] = mu_Reco_pred_test\n",
    "test['GPP_mean'] = mu_GPP_pred_test\n",
    "train['NEE_mean'] = mu_NEE_pred_train\n",
    "train['Reco_mean'] = mu_Reco_pred_train\n",
    "train['GPP_mean'] = mu_GPP_pred_train\n",
    "\n",
    "test['NEE_sigma'] = sigma_NEE_pred_test\n",
    "test['Reco_sigma'] = sigma_Reco_pred_test\n",
    "test['GPP_sigma'] = sigma_GPP_pred_test\n",
    "train['NEE_sigma'] = sigma_NEE_pred_train\n",
    "train['Reco_sigma'] = sigma_Reco_pred_train\n",
    "train['GPP_sigma'] = sigma_GPP_pred_train\n",
    "\n",
    "train_day = train.loc[train.APAR_label == 1,]\n",
    "train_night = train.loc[train.APAR_label == 0,]\n",
    "test_day = test.loc[test.APAR_label == 1,]\n",
    "test_night = test.loc[test.APAR_label == 0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usually the respiration gets the worst performance. I tried different model architectures and this seems to be the \"best\" one (even though it's far from optimal). I think 2 effects are conjugated here:\n",
    "\n",
    "- The respiration has a lower amplitude than the GPP, thus the model primary focuses on the GPP as it's responsible for a larger part of the loss;\n",
    "- The Laplace prior for the model parameters is enforcing too much sparsity, which harms the performances;\n",
    "\n",
    "On the left, the entire training and testing sets are shown (it's not the best as they are quite large, I'll work on that). On the right the MAP estimates are plotted against the observed values.\n",
    "\n",
    "The MAP estimate seems to capture the global trend, but is quite inaccurate. The uncertainty is so high that the predictions are almost unusable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = quad_viz(train, test, \"Reco\", colors=\"Tsoil\", unit=\"(umol m-2 s-1)\", filename=\"../etc/images/reco_nn.png\")\n",
    "print(\"Ratio of training observations outside the 2σ band: {r:.2e}\".format(r=count_out_distribution(train_day, \"Reco\") / train_day.shape[0]))\n",
    "print(\"Ratio of testing observations outside the 2σ band: {r:.2e}\".format(r=count_out_distribution(test_day, \"Reco\") / train_day.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results are much better for the GPP part. Again, the data sets are too large to have a nice time-series plot. But here, the MAP estimates are much more accurate, with a high $R^2$, and when zooming on the test set, the uncertainty seem to be completely reasonable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = quad_viz(train, test, \"GPP\", filename=\"../etc/images/gpp_nn.png\")\n",
    "print(\"Ratio of training observations outside the 2σ band: {r:.2e}\".format(r=count_out_distribution(train_day, \"GPP\") / train_day.shape[0]))\n",
    "print(\"Ratio of testing observations outside the 2σ band: {r:.2e}\".format(r=count_out_distribution(test_day, \"GPP\") / train_day.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For control, here is the NEE trajectory. The performances are good overall, despite the poor results for Reco, which seems to confirm that the modeling choices are giving too much weight to GPP."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = quad_viz(train, test, \"NEE\", filename=\"../etc/images/nee_nn.png\")\n",
    "print(\"Ratio of training observations outside the 2σ band: {r:.2e}\".format(r=count_out_distribution(train_day, \"NEE\") / train_day.shape[0]))\n",
    "print(\"Ratio of testing observations outside the 2σ band: {r:.2e}\".format(r=count_out_distribution(test_day, \"NEE\") / train_day.shape[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## $L_1$ regularization\n",
    "\n",
    "At first, I thought that the Bayesian modeling was enforcing too much sparsity on the parameters. Thus, I tried to train a few NN models with different $L_1$ regularization. It is not as conclusive as I expected, but it's not enough to reject the hypothesis either.\n",
    "\n",
    "Without any regularization, the predictions are much closer than our previous MAP estimate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GPP\n",
    "APAR_input = Input(shape=(1,), dtype='float32', name='APAR_input')\n",
    "EV_input1 = Input(shape=(EV1_train1.shape[1],), dtype='float32', name='EV_input1')\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden1_1')(EV_input1)\n",
    "ln_GPP = Dense(1, activation=None, name='ln_GPP')(x)\n",
    "GPP_1 = Lambda(lambda x: K.exp(x), name='GPP_1')(ln_GPP)\n",
    "GPP = keras.layers.Multiply(name='GPP')([GPP_1, APAR_input])\n",
    "\n",
    "# Reco\n",
    "EV_input2 = Input(shape=(EV2_train1.shape[1],), dtype='float32', name='EV_input2')\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden2_1')(EV_input2)\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden2_2')(x)\n",
    "ln_Reco = Dense(1, activation=None, name='ln_Reco')(x)\n",
    "Reco = Lambda(lambda x: K.exp(x), name='Reco')(ln_Reco)\n",
    "\n",
    "NEE = keras.layers.Subtract(name='NEE')([Reco, GPP])\n",
    "\n",
    "model_NEE = Model(inputs=[APAR_input, EV_input1, EV_input2], outputs=[NEE])\n",
    "model_NEE.compile(\n",
    "    optimizer=keras.optimizers.Adam(2e-3),\n",
    "    loss=keras.losses.MeanSquaredError()\n",
    ")\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "model_NEE.fit({'APAR_input': label_train, 'EV_input1': EV1_train1, 'EV_input2': EV2_train1},\n",
    "                             {'NEE': NEE_train1},\n",
    "                             epochs=500,\n",
    "                             batch_size=64,\n",
    "                             callbacks=[earlyStopping],\n",
    "                             validation_split=0.3,\n",
    "                             verbose=0)\n",
    "NEE_train_NN, GPP_train_NN, Reco_train_NN = fluxes_SIF_predict_noSIF(model_NEE, label_train, EV1_train1, EV2_train1, NEE_max_abs)\n",
    "NEE_test_NN, GPP_test_NN, Reco_test_NN = fluxes_SIF_predict_noSIF(model_NEE, label_test, EV1_test1, EV2_test1, NEE_max_abs)\n",
    "test['NEE_MAP'] = NEE_test_NN\n",
    "test['Reco_MAP'] = Reco_test_NN\n",
    "test['GPP_MAP'] = GPP_test_NN\n",
    "train['NEE_MAP'] = NEE_train_NN\n",
    "train['Reco_MAP'] = Reco_train_NN\n",
    "train['GPP_MAP'] = GPP_train_NN\n",
    "fig, ax = quad_viz(train, test, \"Reco\", colors=\"Tsoil\", unit=\"(umol m-2 s-1)\", bayesian=False)\n",
    "None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With a weight of $10^{-4}$ on all the parameters, performance actually increases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GPP\n",
    "APAR_input = Input(shape=(1,), dtype='float32', name='APAR_input')\n",
    "EV_input1 = Input(shape=(EV1_train1.shape[1],), dtype='float32', name='EV_input1')\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden1_1', kernel_regularizer=tf.keras.regularizers.l1(1e-4), bias_regularizer=tf.keras.regularizers.l1(1e-4))(EV_input1)\n",
    "ln_GPP = Dense(1, activation=None, name='ln_GPP')(x)\n",
    "GPP_1 = Lambda(lambda x: K.exp(x), name='GPP_1')(ln_GPP)\n",
    "GPP = keras.layers.Multiply(name='GPP')([GPP_1, APAR_input])\n",
    "\n",
    "# Reco\n",
    "EV_input2 = Input(shape=(EV2_train1.shape[1],), dtype='float32', name='EV_input2')\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden2_1', kernel_regularizer=tf.keras.regularizers.l1(1e-4), bias_regularizer=tf.keras.regularizers.l1(1e-4))(EV_input2)\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden2_2', kernel_regularizer=tf.keras.regularizers.l1(1e-4), bias_regularizer=tf.keras.regularizers.l1(1e-4))(x)\n",
    "ln_Reco = Dense(1, activation=None, name='ln_Reco')(x)\n",
    "Reco = Lambda(lambda x: K.exp(x), name='Reco')(ln_Reco)\n",
    "\n",
    "NEE = keras.layers.Subtract(name='NEE')([Reco, GPP])\n",
    "\n",
    "model_NEE = Model(inputs=[APAR_input, EV_input1, EV_input2], outputs=[NEE])\n",
    "model_NEE.compile(\n",
    "    optimizer=keras.optimizers.Adam(2e-3),\n",
    "    loss=keras.losses.MeanSquaredError()\n",
    ")\n",
    "model_NEE.fit({'APAR_input': label_train, 'EV_input1': EV1_train1, 'EV_input2': EV2_train1},\n",
    "                             {'NEE': NEE_train1},\n",
    "                             epochs=500,\n",
    "                             batch_size=64,\n",
    "                             callbacks=[earlyStopping],\n",
    "                             validation_split=0.3,\n",
    "                             verbose=0)\n",
    "NEE_train_NN, GPP_train_NN, Reco_train_NN = fluxes_SIF_predict_noSIF(model_NEE, label_train, EV1_train1, EV2_train1, NEE_max_abs)\n",
    "NEE_test_NN, GPP_test_NN, Reco_test_NN = fluxes_SIF_predict_noSIF(model_NEE, label_test, EV1_test1, EV2_test1, NEE_max_abs)\n",
    "test['NEE_MAP'] = NEE_test_NN\n",
    "test['Reco_MAP'] = Reco_test_NN\n",
    "test['GPP_MAP'] = GPP_test_NN\n",
    "train['NEE_MAP'] = NEE_train_NN\n",
    "train['Reco_MAP'] = Reco_train_NN\n",
    "train['GPP_MAP'] = GPP_train_NN\n",
    "fig, ax = quad_viz(train, test, \"Reco\", colors=\"Tsoil\", unit=\"(umol m-2 s-1)\", bayesian=False)\n",
    "None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, with more regularization ($> 3\\times 10^{-4}$) the predictions become almost constant with value 4."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GPP\n",
    "APAR_input = Input(shape=(1,), dtype='float32', name='APAR_input')\n",
    "EV_input1 = Input(shape=(EV1_train1.shape[1],), dtype='float32', name='EV_input1')\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden1_1', kernel_regularizer=tf.keras.regularizers.l1(3e-4), bias_regularizer=tf.keras.regularizers.l1(3e-4))(EV_input1)\n",
    "ln_GPP = Dense(1, activation=None, name='ln_GPP')(x)\n",
    "GPP_1 = Lambda(lambda x: K.exp(x), name='GPP_1')(ln_GPP)\n",
    "GPP = keras.layers.Multiply(name='GPP')([GPP_1, APAR_input])\n",
    "\n",
    "# Reco\n",
    "EV_input2 = Input(shape=(EV2_train1.shape[1],), dtype='float32', name='EV_input2')\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden2_1', kernel_regularizer=tf.keras.regularizers.l1(3e-4), bias_regularizer=tf.keras.regularizers.l1(3e-4))(EV_input2)\n",
    "x = Dense(hidden_dim, activation=\"relu\", name='hidden2_2', kernel_regularizer=tf.keras.regularizers.l1(3e-4), bias_regularizer=tf.keras.regularizers.l1(3e-4))(x)\n",
    "ln_Reco = Dense(1, activation=None, name='ln_Reco')(x)\n",
    "Reco = Lambda(lambda x: K.exp(x), name='Reco')(ln_Reco)\n",
    "\n",
    "NEE = keras.layers.Subtract(name='NEE')([Reco, GPP])\n",
    "\n",
    "model_NEE = Model(inputs=[APAR_input, EV_input1, EV_input2], outputs=[NEE])\n",
    "model_NEE.compile(\n",
    "    optimizer=keras.optimizers.Adam(2e-3),\n",
    "    loss=keras.losses.MeanSquaredError()\n",
    ")\n",
    "model_NEE.fit({'APAR_input': label_train, 'EV_input1': EV1_train1, 'EV_input2': EV2_train1},\n",
    "                             {'NEE': NEE_train1},\n",
    "                             epochs=500,\n",
    "                             batch_size=64,\n",
    "                             callbacks=[earlyStopping],\n",
    "                             validation_split=0.3,\n",
    "                             verbose=0)\n",
    "NEE_train_NN, GPP_train_NN, Reco_train_NN = fluxes_SIF_predict_noSIF(model_NEE, label_train, EV1_train1, EV2_train1, NEE_max_abs)\n",
    "NEE_test_NN, GPP_test_NN, Reco_test_NN = fluxes_SIF_predict_noSIF(model_NEE, label_test, EV1_test1, EV2_test1, NEE_max_abs)\n",
    "test['NEE_MAP'] = NEE_test_NN\n",
    "test['Reco_MAP'] = Reco_test_NN\n",
    "test['GPP_MAP'] = GPP_test_NN\n",
    "train['NEE_MAP'] = NEE_train_NN\n",
    "train['Reco_MAP'] = Reco_train_NN\n",
    "train['GPP_MAP'] = GPP_train_NN\n",
    "fig, ax = quad_viz(train, test, \"Reco\", colors=\"Tsoil\", unit=\"(umol m-2 s-1)\", bayesian=False)\n",
    "None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While it's a nice warning on the effect of over-regularization, predictions don't look anything like our previous MAP predictions. So we cannot conclude on the benefits of the sparsity brought by the Laplace prior."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from HMC import HMC\n",
    "from diagnostic import geweke, gelman_rubin\n",
    "from pathlib import Path\n",
    "\n",
    "tf.random.set_seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pre-training and setup\n",
    "## Creating a dummy model\n",
    "\n",
    "Our model is a basic 4x1 Dense layer, without activation and with $L_1$ regularization. It uses the MSE loss. I added a normalization layer for increased numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(4,)),\n",
    "    keras.layers.Normalization(),\n",
    "    keras.layers.Dense(1, use_bias=True) #, kernel_regularizer=tf.keras.regularizers.l1(0.1))\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-2), loss=keras.losses.MeanSquaredError())\n",
    "keras.utils.plot_model(model, show_shapes=True, to_file=\"../etc/test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We create a trivial training set, with a regular grid $t$ sampled on $[-\\pi, \\pi]$. We set\n",
    "- $x = (t^2, t, \\cos(t), - t^3)$\n",
    "- $y = t$\n",
    "Thus we expect our model to only use the 2nd coordinate of $x$ for its prediction. When accounted for the normalization layer, the final parameters should be $(0,1.85,0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creating fake data\n",
    "batch_size=1000\n",
    "t = np.linspace(-3.14, 3.14, batch_size, dtype=np.float32)\n",
    "x = np.stack([\n",
    "    t ** 2,\n",
    "    t,\n",
    "    np.cos(t),\n",
    "    - t ** 3\n",
    "]).T\n",
    "y = np.expand_dims(t, axis=1)  + 0 * np.random.randn(batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# model pre-training\n",
    "model.layers[0].adapt(x)\n",
    "model.fit(x, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot fitted prediction and model pre-trained parameters\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(t, y, 'ro', label=\"training data\")\n",
    "plt.plot(t, model(x), 'b-', label=\"prediction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After 500 training step, we have a decent fit. Our parameters aren't set to $(0,1.85,0,0)$, but it should be good enough as a pre-training.\n",
    "\n",
    "# HMC\n",
    "## HMC walk\n",
    "\n",
    "We now use the new `HMC` super-model to train our model, and predict uncertainties. It runs in about 20 seconds on my setup (GTX 1660 Ti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "burn_in = 2000\n",
    "n_iter = 2000\n",
    "\n",
    "def sample(model):\n",
    "    # Create HMC super-model\n",
    "    hmc = HMC(model, L=15, epsilon_min=7e-3, epsilon_max=7e-3 , batch_size=batch_size, n_obs=batch_size)\n",
    "\n",
    "    # Initialize bookkeeping\n",
    "    parameters = []  # bookkeeping the parameters\n",
    "    log_gamma = []  # bookkeeping the loggamma\n",
    "    log_lambda = []  # bookkeeping the loggamma\n",
    "    log_likelihood = []  # bookkeeping the loggamma\n",
    "    hamiltonians = []\n",
    "    acceptance = []\n",
    "    inputs = (tf.convert_to_tensor(x, dtype=tf.float32), tf.convert_to_tensor(y, dtype=tf.float32))\n",
    "\n",
    "    # training loop\n",
    "    hmc.init_parameters(inputs)\n",
    "    for step in tqdm(range(n_iter + burn_in)):\n",
    "        new_state, loss, p, accepted, h = hmc(inputs, tf.constant(step, dtype=tf.float32), tf.constant(n_iter, dtype=tf.float32))\n",
    "\n",
    "        # bookkeeping\n",
    "        if step > burn_in:\n",
    "            parameters.append(new_state.position)\n",
    "            log_gamma.append(new_state.log_gamma)\n",
    "            log_lambda.append(new_state.log_lambda)\n",
    "            log_likelihood.append(loss)\n",
    "            acceptance.append(accepted)\n",
    "            hamiltonians.append(h)\n",
    "\n",
    "    parameters = tf.stack(parameters, axis=0).numpy()\n",
    "    log_gamma = tf.concat(log_gamma, axis=0).numpy()\n",
    "    log_lambda = tf.concat(log_lambda, axis=0).numpy()\n",
    "    log_likelihood = tf.concat(log_likelihood, axis=0).numpy()\n",
    "    hamiltonians = tf.concat(hamiltonians, axis=0).numpy()\n",
    "    acceptance = np.array(acceptance)\n",
    "    print(f\"Sampling done: {n_iter} steps with accepting rate {np.mean(acceptance)}\")\n",
    "    return hmc, parameters, log_gamma, log_lambda, log_likelihood, hamiltonians, acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "hmc, parameters, log_gamma, log_lambda, log_likelihood, hamiltonians, acceptance = sample(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## HMC estimates and uncertainties\n",
    "\n",
    "We compute these estimates the same way it was done in previous works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Computing MAP estimate\n",
    "idx_MAP = np.argmin(log_likelihood)\n",
    "MAP = parameters[idx_MAP, :]\n",
    "hmc.set_model_params(MAP)\n",
    "y_MAP = hmc.model(x)\n",
    "\n",
    "# preparing sampling\n",
    "precision = np.exp(log_gamma)\n",
    "num_dim = 1\n",
    "n_samples = 1000\n",
    "trajectories = np.zeros((batch_size, num_dim, n_samples))\n",
    "sigma_normal = np.std(y)\n",
    "\n",
    "# sampling\n",
    "for k in tqdm(range(n_samples)):\n",
    "    idx_1 = np.random.randint(0, n_iter - 1)\n",
    "    idx_2 = np.random.randint(0, n_iter - 1)\n",
    "    w_sample = parameters[-idx_1, :]\n",
    "    precision_here = precision[-idx_2] * num_dim\n",
    "    hmc.set_model_params(w_sample)\n",
    "    trajectories[:, :, k] = hmc.model(x) + sigma_normal * np.random.normal() / np.sqrt(precision_here)\n",
    "\n",
    "mu_pred = np.mean(trajectories, axis=2)\n",
    "sigma_pred = np.var(trajectories, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot results and display MAP parameters\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(t, y, 'ro', label=\"training data\")\n",
    "plt.plot(t, y_MAP, label=\"MAP estimate\")\n",
    "plt.plot(t, mu_pred, label=\"Avg estimate\")\n",
    "lower = mu_pred[:, 0] - 2 * np.sqrt(sigma_pred[:, 0])\n",
    "upper = mu_pred[:, 0] + 2 * np.sqrt(sigma_pred[:, 0])\n",
    "plt.fill_between(t, lower, upper, facecolor=\"orange\", alpha=0.5, label=\"Two std band\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This result shows some uncertainty despite the simplicity of the model. However, the MAP estimate is very close from the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot auto-correlation\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(12,8))\n",
    "axs = axs.flatten()\n",
    "max_lag = 1000\n",
    "for i in range(parameters.shape[1]):\n",
    "    auto_cor = []\n",
    "    mean = np.mean(parameters[:,i])\n",
    "    var = np.var(parameters[:,i])\n",
    "    for lag in range(1,max_lag):\n",
    "        param = parameters[lag:, i]\n",
    "        param_lagged = parameters[:-lag, i]\n",
    "        auto_cor.append(np.mean((param-mean) * (param_lagged-mean))/var)\n",
    "    axs[i].plot(np.arange(1,max_lag), auto_cor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The auto-correlation plots for the 4 parameters show very good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "mean = np.mean(log_gamma)\n",
    "var = np.var(log_gamma)\n",
    "auto_cor = []\n",
    "for lag in range(1,max_lag):\n",
    "    param = log_gamma[lag:]\n",
    "    param_lagged = log_gamma[:-lag]\n",
    "    auto_cor.append(np.mean((param-mean) * (param_lagged-mean))/var)\n",
    "axs[0].plot(np.arange(1,max_lag), auto_cor)\n",
    "\n",
    "mean = np.mean(log_lambda)\n",
    "var = np.var(log_lambda)\n",
    "auto_cor = []\n",
    "for lag in range(1,max_lag):\n",
    "    param = log_lambda[lag:]\n",
    "    param_lagged = log_lambda[:-lag]\n",
    "    auto_cor.append(np.mean((param-mean) * (param_lagged-mean))/var)\n",
    "axs[1].plot(np.arange(1,max_lag), auto_cor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results are also quite good for $\\log(\\gamma)$ and $\\log(\\lambda)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot parameters spread\n",
    "fig, axs = plt.subplots(5, 5, figsize=(15, 15))\n",
    "for i in range(5):\n",
    "    axs[i, i].hist(parameters[:, i], bins=30, density=True)\n",
    "    for j in range(i):\n",
    "        axs[i, j].scatter(parameters[:, j], parameters[:, i], s=1, marker=\"+\", c=np.arange(parameters.shape[0]))\n",
    "    for j in range(i + 1, 5):\n",
    "        axs[i, j].hist2d(parameters[:, j], parameters[:, i], bins=30, cmap=\"Blues\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The parameters seem to be spread according to gaussian distributions, which is expected. Also, there is a high correlation between $t$ and $-t^3$ coefficients, and between $t^2$ and $\\cos(t)$, which is expected. But they seem a bit too widely spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Diagnostic: Geweke z-score\n",
    "print(\"Home-made Geweke test for log(γ): \", geweke(log_gamma)[0])\n",
    "print(\"Home-made Geweke test for log(λ): \", geweke(log_lambda)[0])\n",
    "print(\"Home-made Geweke test for parameters: \", geweke(parameters)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Geweke z-score is decent for all parameters. An official implementation should be used though, as the current one doesn't use spectral estimates of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "model = keras.Sequential([keras.Input(shape=(4,)), keras.layers.Normalization(), keras.layers.Dense(1, use_bias=True)])\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-2), loss=keras.losses.MeanSquaredError())\n",
    "model.layers[0].adapt(x)\n",
    "model.fit(x, y, epochs=500, verbose=0)\n",
    "_, parameters1, log_gamma1, log_lambda1, log_likelihood1, hamiltonians1, acceptance1 = sample(model)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "model = keras.Sequential([keras.Input(shape=(4,)), keras.layers.Normalization(), keras.layers.Dense(1, use_bias=True)])\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-2), loss=keras.losses.MeanSquaredError())\n",
    "model.layers[0].adapt(x)\n",
    "model.fit(x, y, epochs=500, verbose=0)\n",
    "_, parameters2, log_gamma2, log_lambda2, log_likelihood2, hamiltonians2, acceptance2 = sample(model)\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "model = keras.Sequential([keras.Input(shape=(4,)), keras.layers.Normalization(), keras.layers.Dense(1, use_bias=True)])\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-2), loss=keras.losses.MeanSquaredError())\n",
    "model.layers[0].adapt(x)\n",
    "model.fit(x, y, epochs=500, verbose=0)\n",
    "_, parameters3, log_gamma3, log_lambda3, log_likelihood3, hamiltonians3, acceptance3 = sample(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dir = Path(\"../etc/diagnostic/testing\")\n",
    "dir.mkdir(exist_ok=True)\n",
    "\n",
    "np.save(dir / \"log_gamma.npy\", np.stack([log_gamma, log_gamma1, log_gamma2, log_gamma3], axis=1).astype(np.float64))\n",
    "np.save(dir / \"log_lambda.npy\", np.stack([log_lambda, log_lambda1, log_lambda2, log_lambda3], axis=1).astype(np.float64))\n",
    "tmp = np.stack([parameters, parameters1, parameters2, parameters3], axis=2).astype(np.float64)\n",
    "for i in range(tmp.shape[1]):\n",
    "    np.save(dir / (\"parameter\" + str(i) + \".npy\"), tmp[:,i,:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Gelman-Rubin test for log(γ): \", gelman_rubin(np.stack([log_gamma, log_gamma1, log_gamma2, log_gamma3], axis=1)))\n",
    "print(\"Gelman-Rubin test for log(λ): \", gelman_rubin(np.stack([log_lambda, log_lambda1, log_lambda2, log_lambda3], axis=1)))\n",
    "print(\"Gelman-Rubin test for parameters: \", gelman_rubin(np.stack([parameters, parameters1, parameters2, parameters3], axis=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This time, the Gelman-Rubin test is very conclusive for all our weights and for $\\log(\\gamma)$ and $\\log(\\lambda)$ as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}